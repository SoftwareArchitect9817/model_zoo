python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_256x256.onnx \
 --input_shape [1,3,256,256] \
 --output_dir openvino/256x256/FP32 \
 --data_type FP32

python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_256x256.onnx \
 --input_shape [1,3,256,256] \
 --output_dir openvino/256x256/FP16 \
 --data_type FP16



python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_320x320.onnx \
 --input_shape [1,3,320,320] \
 --output_dir openvino/320x320/FP32 \
 --data_type FP32

python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_320x320.onnx \
 --input_shape [1,3,320,320] \
 --output_dir openvino/320x320/FP16 \
 --data_type FP16



python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_480x640.onnx \
 --input_shape [1,3,480,640] \
 --output_dir openvino/480x640/FP32 \
 --data_type FP32

python3 ${INTEL_OPENVINO_DIR}/deployment_tools/model_optimizer/mo.py \
 --input_model u2netp_480x640.onnx \
 --input_shape [1,3,480,640] \
 --output_dir openvino/480x640/FP16 \
 --data_type FP16


cd ${HOME}/inference_engine_cpp_samples_build/intel64/Release

./benchmark_app \
  -m ${HOME}/git/U-2-Net/openvino/480x640/FP16/u2netp_480x640.xml \
  -i ${HOME}/Pictures \
  -d CPU \
  -nthreads 12

./benchmark_app \
  -m ${HOME}/git/U-2-Net/openvino/256x256/FP16/u2netp_256x256.xml \
  -i ${HOME}/Pictures \
  -d CPU \
  -nthreads 12



